{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ykNZWbhetbP"
      },
      "source": [
        "# SI 670 Applied Machine Learning, Homework 3\n",
        "\n",
        "Please export this notebook as pdf with file name `si670f25_hw3_youruniqname.pdf` when submitting on gradescope.\n",
        "\n",
        "As a reminder, the work you submit must be your own work. Feel free to discuss general approaches to the homework with classmates: if you end up forming more of a team discussion on multiple questions, please include the names of the people you worked with at the top of your notebook file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZjVHY0ve2NF"
      },
      "source": [
        "**Your name:**\n",
        "\n",
        "**Your uniqname:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chZeS_jeg1RQ"
      },
      "source": [
        "### Setup\n",
        "In this assignment you will train several linear classifier models and evaluate how effectively they predict instances of fraud using data based on [this dataset from Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud). Then you'll perform a grid search to find optimal parameters.\n",
        "\n",
        "Each row in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` through `V28` as well as `Amount` which is the amount of the transaction.\n",
        "\n",
        "The target is stored in the `class` column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1wVYTX8ChJnh"
      },
      "outputs": [],
      "source": [
        "# # run this cell if you are using Jupyter\n",
        "files = {'fraud_data.csv': 'fraud_data.csv'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qMuN4twnidD-"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(files['fraud_data.csv'])\n",
        "\n",
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHGcnVBYgTIe"
      },
      "source": [
        "## Question 1 (10 points)\n",
        "\n",
        "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
        "\n",
        "Then train a LogisticRegression classifier with C=1. What is the accuracy? What is the recall?\n",
        "\n",
        "*This function should a return a tuple with four floats, i.e. `(dummy_accuracy, dummy_recall, lr_accuracy, lr_recall)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NO4nbZ4exzC"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def answer_one():\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    dummy = DummyClassifier(strategy = 'most_frequent')\n",
        "    dummy_accuracy = # YOUR CODE HERE\n",
        "    dummy_recall = # YOUR CODE HERE\n",
        "\n",
        "    lr = LogisticRegression(C=1)\n",
        "\n",
        "    lr_accuracy = # YOUR CODE HERE\n",
        "    lr_recall = # YOUR CODE HERE\n",
        "\n",
        "    return dummy_accuracy, dummy_recall, lr_accuracy, lr_recall\n",
        "\n",
        "answer_one()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f12byghginP2"
      },
      "source": [
        "## Question 2 (10 points)\n",
        "\n",
        "Fit the LogisticRegression with `C` varying from `[[0.1, 1, 10]` and report the accuracy, precision, recall, and F1 scores for each choice of `C`.\n",
        "\n",
        "*This function should a return a tuple with four lists, i.e. `(accuracy_list, precision_list, recall_list, f1_list)`, and each list should contain 3 numbers.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fIuruqDiAKf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def answer_two():\n",
        "\n",
        "    # Accuracy = TP + TN / (TP + TN + FP + FN)\n",
        "    # Precision = TP / (TP + FP)\n",
        "    # Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
        "    # F1 = 2 * Precision * Recall / (Precision + Recall)\n",
        "\n",
        "    accuracy_list = []\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    f1_list = []\n",
        "    for this_C in [0.1, 1, 10]:\n",
        "        lr = LogisticRegression(C=this_C)\n",
        "        # YOUR CODE HERE\n",
        "    return accuracy_list, precision_list, recall_list, f1_list\n",
        "\n",
        "answer_two()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O_EFV0-j4yF"
      },
      "source": [
        "## Question 3 (10 points)\n",
        "\n",
        "In class, we saw how using time of the day as is as a feature creates trouble for us because of a sharp discontinuity. Our solution was to use the sine and cosine functions to create two new features. Briefly explain why using only sine or only cosine would not be sufficient as a solution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA9QF6ACmjaW"
      },
      "source": [
        "**Your answer to Question 3 here**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMJ1FKC9kD_S"
      },
      "source": [
        "## Question 4 (20 points)\n",
        "You are evaluating some TikTok videos to see if they express pro-Harris or pro-Trump attitudes during the presidential election. Given the confusion matrix below, please\n",
        "\n",
        "(1) just look at the confusion matrix without calculating, summarize the main problem with this classifier.\n",
        "\n",
        "(2) first compute the precision, recall, FNR, FPR, F1 score and accuracy for each class, then compute the micro-average of these metrics.\n",
        "\n",
        "| Actual label | Predicted Pro-Trump | Predicted Pro-Harris | Predicted Neutral |\n",
        "|--------------|---------------|---------------|------------------|\n",
        "| Pro-Trump          | 30            | 80            | 33               |\n",
        "| Pro-Harris          | 10            | 70            | 29               |\n",
        "| Neutral       | 52            | 27            | 49               |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsnXHSpnuwGk"
      },
      "source": [
        "**Your answer to Question 4 here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v9vqO0Bk6jf"
      },
      "source": [
        "## Question 5 (10 points)\n",
        "Examine the code snippet provided below. Identify all the data leakage in it, and explain how to fix the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-Wdd0l0itYa"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_cancer, y_cancer = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_cancer_scaled = scaler.fit_transform(X_cancer)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cancer_scaled, y_cancer, random_state=0)\n",
        "\n",
        "lr = LogisticRegression().fit(X_train, y_train)\n",
        "print('Breast cancer dataset')\n",
        "print('Logistic regression accuracy score: {:.3f}'.format(lr.score(X_test, y_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYUjVw1wmvC_"
      },
      "source": [
        "**Your answer to Question 5 here**\n",
        "\n",
        "Normalizes all of the data at the same time in line 8. Instead, data should be split into training and test first, and then normalized separately. Also, the scaler should only be fit to training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAbhZPeql-En"
      },
      "source": [
        "## Question 6 (10 points)\n",
        "\n",
        "You are trying to build a new and effective ML model for Michigan Medicine that can detect breast cancer from patient data. You evaluate your new model using accuracy as a metric, and find that it achieves good accuracy.\n",
        "\n",
        "However, your colleague from Michigan Medicine suggests that instead of maximizing overall accuracy, you should be minimizing the false negative rate (FNR). Briefly explain why neither of these criteria are a good idea to optimize against.  Name two criteria that might work better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prr6xdfLm4n9"
      },
      "source": [
        "**Your answer to Question 6 here**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv2TSbJFmGu2"
      },
      "source": [
        "## Question 7 (20 points)\n",
        "\n",
        "(1) Suppose that you are trying to solve a binary classification problem (labels 0 or 1), and you have a soft classifier that, for each data point, outputs the probability that the data point has label 1. The table below shows this probability for each data point, along with the true label. Please calculate the respective points on the ROC curve.\n",
        "\n",
        "| Prediction Probability | 0.3 | 0.4 | 0.5 | 0.5 | 0.75 |\n",
        "|------------------------|-----|-----|-----|-----|------|\n",
        "| True label             | 0   | 1   | 0   | 1   | 1    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSj0T0bunbBb"
      },
      "source": [
        "(2) Now implement the ROC curve in python code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRIg4Ji_rXH9"
      },
      "source": [
        "**Your answer to Question 7(1) here**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nSFXI9ul9FH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# True labels and predicted probabilities\n",
        "y_true = [0, 1, 0, 1, 1]\n",
        "y_scores = [0.3, 0.4, 0.5, 0.5, 0.75]\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "\n",
        "roc_auc = # Your code here\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, marker='o', label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "322MBWN2pqvY"
      },
      "source": [
        "## Question 8 (10 points)\n",
        "\n",
        "Finish the following code to implement a 5-fold cross validation, and return\n",
        "\n",
        "(1) the best parameters\n",
        "\n",
        "(2) the best CV accuracy\n",
        "\n",
        "(3) the best test precision, recall, and F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxlRhlfxl4ce"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load and split dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Logistic Regression with GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],   # Regularization strength\n",
        "    'penalty': ['l2'],              # 'l1' requires solver='liblinear' or 'saga'\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "grid_search = # YOUR CODE HERE\n",
        "\n",
        "\n",
        "print(\"Best parameters:\", # YOUR CODE HERE  )\n",
        "print(\"Best CV accuracy:\", # YOUR CODE HERE  )\n",
        "print(\"Test Accuracy:\", # YOUR CODE HERE  )\n",
        "print(\"Test Precision:\", # YOUR CODE HERE  )\n",
        "print(\"Test Recall:\", # YOUR CODE HERE  )\n",
        "print(\"Test F1:\", # YOUR CODE HERE  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8_A9GmbL99F"
      },
      "source": [
        "## Question 9 (30 points)\n",
        "\n",
        "In this question, you will work with a small dataset of text reviews. Your goal is to build and analyze a sentiment classifier. Complete the following tasks:\n",
        "\n",
        "Build a pipeline using `CountVectorizer` (or `TfidfVectorizer)` and `LogisticRegression` to classify each review as positive or negative.\n",
        "\n",
        "Perform hyperparameter tuning with `GridSearchCV`. Explore different settings for both the vectorizer (e.g., n-grams, minimum document frequency, type of vectorizer) and the logistic regression classifier (e.g., regularization strength).\n",
        "\n",
        "Evaluate your model on a test set. Report performance metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Inspect the top features learned by your model. Identify which words are most strongly associated with positive sentiment and which with negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3na0WXfZrBQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "texts = [\n",
        "    # Positive\n",
        "    \"I love this product, it's amazing!\",\n",
        "    \"Best purchase I’ve ever made.\",\n",
        "    \"Really happy with the quality.\",\n",
        "    \"Superb item, works perfectly.\",\n",
        "    \"Excellent experience, highly recommend.\",\n",
        "    \"The service was outstanding and friendly.\",\n",
        "    \"Totally worth the money.\",\n",
        "    \"Great value and fantastic build.\",\n",
        "    \"I enjoy using it every day.\",\n",
        "    \"Very satisfied with my order.\",\n",
        "\n",
        "    # Negative\n",
        "    \"Absolutely terrible, I hate it.\",\n",
        "    \"Worst experience ever.\",\n",
        "    \"I will never buy this again.\",\n",
        "    \"Cheap and broke after one use.\",\n",
        "    \"Horrible service, very disappointed.\",\n",
        "    \"Waste of money, not recommended.\",\n",
        "    \"The quality is awful.\",\n",
        "    \"Extremely frustrating experience.\",\n",
        "    \"It stopped working within a week.\",\n",
        "    \"Wouldn’t suggest it to anyone.\"\n",
        "]\n",
        "labels = [\n",
        "    1,1,1,1,1,1,1,1,1,1,   # positive\n",
        "    0,0,0,0,0,0,0,0,0,0    # negative\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zosh-hQ-MHd8"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------\n",
        "# Step 1: Train Test Split\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# TODO: Split into train/test sets (use 70/30 split, random_state=42)\n",
        "# Hint: use train_test_split\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Step 2: Pipeline\n",
        "# ---------------------------------------------------\n",
        "# TODO: Define a pipeline that includes:\n",
        "#   - A vectorizer (either CountVectorizer or TfidfVectorizer)\n",
        "#   - A LogisticRegression classifier\n",
        "# Use step names \"vect\" and \"logreg\".\n",
        "pipe = Pipeline([\n",
        "    ...\n",
        "])\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Step 3: Parameter Grid\n",
        "# ---------------------------------------------------\n",
        "# TODO: Create a parameter grid for GridSearchCV that explores:\n",
        "#   - CountVectorizer vs. TfidfVectorizer\n",
        "#   - ngram_range = (1,1) and (1,2)\n",
        "#   - min_df = 1 and 2\n",
        "#   - LogisticRegression C = [0.01, 0.1, 1, 10]\n",
        "#   - LogisticRegression penalty = 'l2'\n",
        "#   - LogisticRegression solver = 'lbfgs'\n",
        "param_grid = [\n",
        "    {\n",
        "        ...\n",
        "    }\n",
        "]\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Step 4: Grid Search\n",
        "# ---------------------------------------------------\n",
        "# TODO: Initialize a GridSearchCV with:\n",
        "#   - estimator = pipe\n",
        "#   - param_grid = param_grid\n",
        "#   - cv = 3\n",
        "#   - scoring = \"f1\"\n",
        "#   - n_jobs = -1\n",
        "grid = ...\n",
        "\n",
        "# TODO: Fit the grid search on the training data\n",
        "...\n",
        "\n",
        "# TODO: Print the best parameters and CV score\n",
        "print(\"Best parameters:\", ...)\n",
        "print(\"Best CV score:\", ...)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Step 5: Evaluation\n",
        "# ---------------------------------------------------\n",
        "# TODO: Predict on the test set and print classification report\n",
        "y_pred = ...\n",
        "print(\"\\nTest set performance:\")\n",
        "print(classification_report(...))\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Step 6: Feature Inspection\n",
        "# ---------------------------------------------------\n",
        "# TODO: Extract the vectorizer and classifier from the best model\n",
        "#       Then print the top 10 positive and top 10 negative features.\n",
        "best_model = ...\n",
        "vectorizer = ...\n",
        "clf = ...\n",
        "\n",
        "feature_names = ...\n",
        "coefs = ...\n",
        "\n",
        "top_pos_idx = ...\n",
        "top_neg_idx = ...\n",
        "\n",
        "print(\"\\nTop positive features:\")\n",
        "for idx in ...:\n",
        "    print(f\"{feature_names[idx]}: {coefs[idx]:.3f}\")\n",
        "\n",
        "print(\"\\nTop negative features:\")\n",
        "for idx in ...:\n",
        "    print(f\"{feature_names[idx]}: {coefs[idx]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0HVVOv6MP4l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "conda_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
